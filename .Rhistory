logistic = logistic_pred,
tree = Tree_pred,
randomForest = randomForest_pred
)
class(evaluation$logistic)
class(evaluation$tree)
class(evaluation$randomForest)
R2(evaluation$actual, evaluation$logistic)
R2(evaluation$actual, evaluation$tree)
R2(evaluation$actual, evaluation$randomForest)
R2(evaluation$actual, evaluation$logistic)
R2(evaluation$actual, evaluation$tree)
R2(evaluation$actual, evaluation$randomForest)
logReg_R2 <- R2(evaluation$actual, evaluation$logistic)
Tree_R2 <- R2(evaluation$actual, evaluation$tree)
randomForest_R2 <- R2(evaluation$actual, evaluation$randomForest)
R2s <- data.frame(
logReg_R2 = R2(evaluation$actual, evaluation$logistic),
Tree_R2 = R2(evaluation$actual, evaluation$tree),
randomForest_R2 = R2(evaluation$actual, evaluation$randomForest))
R2s
R2s <- data.frame(
logReg_R2 = R2(evaluation$actual, evaluation$logistic),
Tree_R2 = R2(evaluation$actual, evaluation$tree),
randomForest_R2 = R2(evaluation$actual, evaluation$randomForest)) %>%
pivot_longer(names_to = model, values_to = R2)
R2s <- data.frame(
logReg_R2 = R2(evaluation$actual, evaluation$logistic),
Tree_R2 = R2(evaluation$actual, evaluation$tree),
randomForest_R2 = R2(evaluation$actual, evaluation$randomForest)) %>%
pivot_longer(names_to = "model", values_to = "R2")
R2s <- data.frame(
logReg_R2 = R2(evaluation$actual, evaluation$logistic),
Tree_R2 = R2(evaluation$actual, evaluation$tree),
randomForest_R2 = R2(evaluation$actual, evaluation$randomForest)) %>%
pivot_longer(col = 1:3, names_to = "model", values_to = "R2")
R2s
ggplot(data = R2s)+
geom_col(aes(x = model, y = R2))
library(glmnet)
Mx<- model.matrix(Default ~ .^2, data=data_clean)[,-1]
Mx
View(Mx)
View(evaluation)
R2s <- data.frame(
logReg_R2 = R2(evaluation$actual, evaluation$logistic),
Tree_R2 = R2(evaluation$actual, evaluation$tree),
randomForest_R2 = R2(evaluation$actual, evaluation$randomForest)) %>%
pivot_longer(col = 1:3, names_to = "model", values_to = "R2")
View(R2s)
#LinearModel
linearModel <- glm(Obama_margin_percent ~., data = train_data, family = "binomial")
#LinearModel
linearModel <- glm(Default ~., data = train_data, family = "binomial")
LM_inSample <- as.numeric(predict(linearModel, newdata = train_data))
LM_pred <- as.numeric(predict(linearModel, newdata = test_data))
LM_perform <- cbind(test_data, LM_pred) %>% mutate(LMdiff = LM_pred-Default)
LM_perform
logistic_pred <- predict(logisticRegression, newdata = test_data)
logistic_pred <- predict(logisticRegression, newdata = test_data, type = "reponse")
logistic_pred <- predict(logisticRegression, newdata = test_data, type = "response")
## need log odd trans
# logistic_pred <- 1/(1+exp(-logistic_pred))
logistic_pred
## need log odd trans
# logistic_pred <- 1/(1+exp(-logistic_pred))
logistic_pred <- ifelse(logistic_pred > 0.5, 1, 0)
logistic_pred
logistic_pred2 <- predict(logisticRegression, newdata = test_data, type = "response")
## need log odd trans
logistic_pred2 <- 1/(1+exp(-logistic_pred2))
logistic_pred2 <- ifelse(logistic_pred2 > 0.5, 1, 0)
logistic_pred2 <- predict(logisticRegression, newdata = test_data)
## need log odd trans
logistic_pred2 <- 1/(1+exp(-logistic_pred2))
logistic_pred2 <- ifelse(logistic_pred2 > 0.5, 1, 0)
logistic_pred = logistic_pred2
logisticRegression <- glm(Default~., data = train_data, family = "binomial")
logisticRegression <- glm(Default~., data = train_data, family = "binomial")
logistic_pred <- predict(logisticRegression, newdata = test_data, type = "response")
logistic_pred2 <- predict(logisticRegression, newdata = test_data)
## need log odd trans
logistic_pred2 <- 1/(1+exp(-logistic_pred2))
logistic_pred2 <- ifelse(logistic_pred2 > 0.5, 1, 0)
logistic_pred <- ifelse(logistic_pred > 0.5, 1, 0)
logistic_pred - logistic_pred2
unique(logistic_pred - logistic_pred2)
My<- data_clean$Default == 1
num.features <- ncol(Mx)
num.n <- nrow(Mx)
num.default <- sum(My)
w <- (num.churn/num.n)*(1-(num.churn/num.n))
w <- (num.default/num.n)*(1-(num.default/num.n))
num.features <- ncol(Mx)
num.n <- nrow(Mx)
num.default <- sum(My)
w <- (num.default/num.n)*(1-(num.default/num.n))
lambda.theory <- sqrt(w*log(num.features/0.05)/num.n)
lassoTheory <- glmnet(Mx,My, family="binomial",lambda = lambda.theory)
summary(lassoTheory)
support(lassoTheory$beta)
colnames(Mx)[support(lassoTheory$beta)]
### there are in total
length(support(lassoTheory$beta))
### coefficients selected by the model using the theoretical choice
###
###
### If we omit the lambda, the function will solve for all values of lambda
### it takes a bit longer but not much longer at all.
lasso <- glmnet(Mx,My, family="binomial")
### coefficients selected by the model using the theoretical choice
###
###
### If we omit the lambda, the function will solve for all values of lambda
### it takes a bit longer but not much longer at all.
lasso <- glmnet(Mx,My, family="binomial")
### there are in total
length(support(lassoTheory$beta))
support(lassoTheory$beta)
colnames(Mx)[support(lassoTheory$beta)]
### there are in total
length(support(lassoTheory$beta))
### coefficients selected by the model using the theoretical choice
###
###
### If we omit the lambda, the function will solve for all values of lambda
### it takes a bit longer but not much longer at all.
lasso <- glmnet(Mx,My, family="binomial")
### coefficients selected by the model using the theoretical choice
###
###
### If we omit the lambda, the function will solve for all values of lambda
### it takes a bit longer but not much longer at all.
lasso <- glmnet(Mx,My, family="binomial")
summary(lasso)
summary(lasso)
### there are in total
length(support(lassoTheory$beta))
summary(lasso)
lasso$lambda[1:5]
ar(mar=c(1.5,1.5,0.75,1.5))
par(mai=c(1.5,1.5,0.75,1.5))
## Make 2 plots side by side: mfrow=c(1,2)  # c(nrow,ncol)
par(mfrow=c(1,2))
ar(mar=c(1.5,1.5,0.75,1.5))
ar(mar=c(1.5,1.5,0.75,1.5))
par(mar=c(1.5,1.5,0.75,1.5))
par(mai=c(1.5,1.5,0.75,1.5))
## Make 2 plots side by side: mfrow=c(1,2)  # c(nrow,ncol)
par(mfrow=c(1,2))
coef_ind <- 5
par(mar=c(1.5,0.5,0.75,0.5))
par(mai=c(1.5,0.5,0.75,0.5))
plot(log(lasso$lambda),lasso$beta[coef_ind,], ylab="Coefficient value", main=paste("Coefficient for",colnames(Mx)[coef_ind]),xlab = expression(paste("log(",lambda,")")),type="l")
coef_ind <- 2
par(mar=c(1.5,0.5,0.75,0.5))
par(mai=c(1.5,0.5,0.75,0.5))
plot(log(lasso$lambda),lasso$beta[coef_ind,], ylab="Coefficient value", main=paste("Coefficient for",colnames(Mx)[coef_ind]),xlab = expression(paste("log(",lambda,")")),type="l")
### There are some rules that people like to use:
### The minimum of the mean values stored in lambda.min
### 1se to the right stored in lambda.1se
### if we had to compute lambda.min we can simply write
lassoCV$lambda[which.min(lassoCV$cvm)]
### Now that we can actually compute the Lasso for all values of lambda,
### the whole "path" of models can be evaluated by a OOS experiment
### we can attempt to use cross valiadation to actually pick lambda.
### the following command yields a cross validation procedure
### the following command takes some time.
lassoCV <- cv.glmnet(Mx,My, family="binomial")
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
### There are some rules that people like to use:
### The minimum of the mean values stored in lambda.min
### 1se to the right stored in lambda.1se
### if we had to compute lambda.min we can simply write
lassoCV$lambda[which.min(lassoCV$cvm)]
### sclassoCV$cvm has the mean values
### which.min(sclassoCV$cvm) returns the index that minimizes it
### and remember that sclassoCV$lambda is the vector of lambda
### in any case we have lambda.min and lambda.1se.
###
### lambda.min is perceived as aggressive (picks too many variables)
### lambda.1se is perceived as conservative (picks too few variables)
###
### Btw, where do you think the Theoretical one stands?
### we plot them in the previous picture
text(log(lassoCV$lambda.min), .95,"min",cex=1)
text(log(lassoCV$lambda.1se), 1,"1se",cex=1)
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
lines(c(log(lambda.theory),log(lambda.theory)),c(0.3,2.4),lty=3,col="blue")
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
### There are some rules that people like to use:
### The minimum of the mean values stored in lambda.min
### 1se to the right stored in lambda.1se
### if we had to compute lambda.min we can simply write
lassoCV$lambda[which.min(lassoCV$cvm)]
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
randomForest <- randomForest(Default~., data = train_data)
# 創建 Lasso 模型
lasso <- cv.glmnet(x = X_train, y = Y_train, alpha = 1, lambda = 0.005)
library(glmnet)
# 創建 Lasso 模型
lasso <- cv.glmnet(x = X_train, y = Y_train, alpha = 1, lambda = 0.005)
# 創建 Lasso 模型
lasso <- cv.glmnet(x = Mx, y = My, alpha = 1, lambda = 0.005)
plot(log(lasso$lambda),lasso$beta[coef_ind,], ylab="Coefficient value", main=paste("Coefficient for",colnames(Mx)[coef_ind]),xlab = expression(paste("log(",lambda,")")),type="l")
plot(log(lasso$lambda),lasso$beta[coef_ind,], ylab="Coefficient value", main=paste("Coefficient for",colnames(Mx)[coef_ind]),xlab = expression(paste("log(",lambda,")")),type="l")
### coefficients selected by the model using the theoretical choice
###
###
### If we omit the lambda, the function will solve for all values of lambda
### it takes a bit longer but not much longer at all.
lasso <- glmnet(Mx,My, family="binomial")
### coefficients selected by the model using the theoretical choice
###
###
### If we omit the lambda, the function will solve for all values of lambda
### it takes a bit longer but not much longer at all.
lasso <- glmnet(Mx,My, family="binomial")
plot(log(lasso$lambda),lasso$beta[coef_ind,], ylab="Coefficient value", main=paste("Coefficient for",colnames(Mx)[coef_ind]),xlab = expression(paste("log(",lambda,")")),type="l")
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
### There are some rules that people like to use:
### The minimum of the mean values stored in lambda.min
### 1se to the right stored in lambda.1se
### if we had to compute lambda.min we can simply write
lassoCV$lambda[which.min(lassoCV$cvm)]
lasso$lambda[1:5]
lasso <- cv.glmnet(x = Mx, y = My, alpha = 1, lambda = 0.005)
lasso <- cv.glmnet(x = Mx, y = My, alpha = 1, lambda = lambda.theory)
lasso <- glmnet(x = Mx, y = My, alpha = 1, lambda = lambda.theory)
lassoTheory <- glmnet(x = Mx, y = My, alpha = 1, lambda = lambda.theory)
lassoTheory <- glmnet(Mx,My, family="binomial",lambda = lambda.theory)
# 找到選定的特徵
selected_features <- which(coef(lassoTheory) != 0)
# 獲取選定的特徵列名稱
selected_col_names <- colnames(Mx)[selected_features]
# 輸出選定的特徵列名稱
selected_col_names
Mx<- model.matrix(Default ~ ., data=data_clean)[,-1]
My<- data_clean$Default == 1
num.features <- ncol(Mx)
num.n <- nrow(Mx)
num.default <- sum(My)
w <- (num.default/num.n)*(1-(num.default/num.n))
lambda.theory <- sqrt(w*log(num.features/0.05)/num.n)
lassoTheory <- glmnet(Mx,My, family="binomial",lambda = lambda.theory)
lassoTheory <- glmnet(x = Mx, y = My, alpha = 1, lambda = lambda.theory)
# 找到選定的特徵
selected_features <- which(coef(lassoTheory) != 0)
# 獲取選定的特徵列名稱
selected_col_names <- colnames(Mx)[selected_features]
lassoTheory <- glmnet(Mx,My, family="binomial",lambda = lambda.theory)
# 找到選定的特徵
selected_features <- which(coef(lassoTheory) != 0)
# 獲取選定的特徵列名稱
selected_col_names <- colnames(Mx)[selected_features]
# 輸出選定的特徵列名稱
selected_col_names
print(selected.variables)
lasso.model <- glmnet(x, y, alpha=1)
cv.lasso <- cv.glmnet(x, y, alpha=1)
optimal.lambda <- cv.lasso$lambda.min
```{r}
lasso.model <- glmnet(Mx, My, alpha=1)
cv.lasso <- cv.glmnet(Mx, My, alpha=1)
optimal.lambda <- cv.lasso$lambda.min
coef.lasso <- coef(lasso.model, s=optimal.lambda)
selected.variables <- rownames(coef.lasso)[which(coef.lasso[, 1] != 0)]
print(selected.variables)
Mx<- model.matrix(Default ~ ., data=data_clean)[,-1]
My<- data_clean$Default == 1
# 輸出選定的特徵列名稱
selected_col_names
lasso.model <- glmnet(Mx, My, alpha=1)
cv.lasso <- cv.glmnet(Mx, My, alpha=1)
cv.lasso <- cv.glmnet(Mx, My, alpha=1)
optimal.lambda <- cv.lasso$lambda.min
length(data_clean)
data_clean
print(selected.variables)
summary(lassoTheory)
# 輸出選定的特徵列名稱
selected_col_names
### There are some rules that people like to use:
### The minimum of the mean values stored in lambda.min
### 1se to the right stored in lambda.1se
### if we had to compute lambda.min we can simply write
lassoCV$lambda[which.min(lassoCV$cvm)]
My<- data_clean$Default
num.features <- ncol(Mx)
num.n <- nrow(Mx)
num.default <- sum(My)
w <- (num.default/num.n)*(1-(num.default/num.n))
lambda.theory <- sqrt(w*log(num.features/0.05)/num.n)
lassoTheory <- glmnet(Mx,My, family="binomial",lambda = lambda.theory)
# 找到選定的特徵
selected_features <- which(coef(lassoTheory) != 0)
# 獲取選定的特徵列名稱
selected_col_names <- colnames(Mx)[selected_features]
# 輸出選定的特徵列名稱
selected_col_names
summary(lassoTheory)
# 找到選定的特徵
selected_features <- which(coef(lassoTheory) != 0)
# 獲取選定的特徵列名稱
selected_col_names <- colnames(Mx)[selected_features]
# 輸出選定的特徵列名稱
selected_col_names
support(lassoTheory$beta)
colnames(Mx)[support(lassoTheory$beta)]
### there are in total
length(support(lassoTheory$beta))
### Now that we can actually compute the Lasso for all values of lambda,
### the whole "path" of models can be evaluated by a OOS experiment
### we can attempt to use cross valiadation to actually pick lambda.
### the following command yields a cross validation procedure
### the following command takes some time.
lassoCV <- cv.glmnet(Mx,My, family="binomial")
lassoCV$lambda.min
optimal_lambda <- lassoCV$lambda.min
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
optimal_lambda <- lassoCV$lambda.min
coef_matrix <- coef(lassoTheory, s = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
### there are in total
length(support(lassoTheory$beta))
lasso_min <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
lasso_min <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
coef_matrix <- coef(lasso_min, s = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
Mx<- model.matrix(Default ~ ., data=data_clean)[,-1]
My<- data_clean$Default
num.features <- ncol(Mx)
num.n <- nrow(Mx)
num.default <- sum(My)
w <- (num.default/num.n)*(1-(num.default/num.n))
lambda.theory <- sqrt(w*log(num.features/0.05)/num.n)
lassoTheory <- glmnet(Mx,My, family="binomial",lambda = lambda.theory)
lassoTheory <- glmnet(Mx,My, family="binomial",lambda = lambda.theory)
summary(lassoTheory)
summary(lassoTheory)
support(lassoTheory$beta)
colnames(Mx)[support(lassoTheory$beta)]
### there are in total
length(support(lassoTheory$beta))
lasso <- glmnet(Mx,My, family="binomial")
lasso <- glmnet(Mx,My, family="binomial")
summary(lasso)
summary(lasso)
lassoCV <- cv.glmnet(Mx,My, family="binomial")
coef_matrix <- coef(lasso_optLambda, alpha = 1)
coef_matrix <- coef(lasso_optLambda, alpha = 1)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
length(support(lasso_optLambda$beta))
select_variables
optimal_lambda <- lassoCV$lambda.min
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)]
select_variables
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
length(support(lasso_optLambda$beta))
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
lassoCV <- cv.glmnet(Mx,My, family="binomial", alpha = 1)
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
optimal_lambda <- lassoCV$lambda.min
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
length(support(lasso_optLambda$beta))
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
optimal_lambda <- lassoCV$lambda.min
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
length(support(lasso_optLambda$beta))
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
names(data_clean)
select_variables
names(data_clean)
names(data_clean)
select_variables
names(data_clean)
names(data_clean)
select_variables
names(data_clean)
select_variables
names(data_clean)
select_variables
names(data_clean)
select_variables
names(data_clean)
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
optimal_lambda <- lassoCV$lambda.min
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
length(support(lasso_optLambda$beta))
length(support(lasso_optLambda$beta))
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
length(support(lasso_optLambda$beta))
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
length(support(lasso_optLambda$beta))
select_variables <- rownames(coef_matrix)[which(abs(coef_matrix != 0))[-1]]
select_variables <- rownames(coef_matrix)[which(abs(coef_matrix) != 0)[-1]]
select_variables
select_variables <- rownames(coef_matrix)[which(abs(coef_matrix) != 0)[-1]]
select_variables
select_variables <- rownames(coef_matrix)[which(abs(coef_matrix) != 0)[-1]]
select_variables
coef_matrix
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
support(lasso_optLambda$beta)
support(lasso_optLambda$beta)
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
coef_matrix
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
lassoCV <- cv.glmnet(Mx,My, family="binomial")
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
optimal_lambda <- lassoCV$lambda.min
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
optimal_lambda <- lassoCV$lambda.min
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
length(support(lasso_optLambda$beta))
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
length(support(lasso_optLambda$beta))
select_variables
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
optimal_lambda <- lassoCV$lambda.min
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
mregkemrgs
select_variables
optimal_lambda <- lassoCV$lambda.min
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
length(support(lasso_optLambda$beta))
length(support(lasso_optLambda$beta))
coef_matrix <- coef(lasso_optLambda, s = optimal_lambda)
select_variables <- rownames(coef_matrix)[which(coef_matrix != 0)[-1]]
select_variables
logisticRegression <- glm(Default~.-LoanTerm, data = train_data, family = "binomial")
logistic_pred <- predict(logisticRegression, newdata = test_data, type = "response")
logistic_pred <- ifelse(logistic_pred > 0.5, 1, 0)
logisticRegression_lasso <- glm(Default~.-LoanTerm, data = train_data, family = "binomial")
logisticRegression_lasso <- glm(Default~.-LoanTerm, data = train_data, family = "binomial")
LassoLogistic_pred <- predict(logisticRegression, newdata = test_data, type = "response")
LassoLogistic_pred <- predict(logisticRegression, newdata = test_data, type = "response")
LassoLogistic_pred <- ifelse(logistic_pred > 0.5, 1, 0)
logisticRegression <- glm(Default~., data = train_data, family = "binomial")
logisticRegression <- glm(Default~., data = train_data, family = "binomial")
logistic_pred <- predict(logisticRegression, newdata = test_data, type = "response")
logistic_pred <- predict(logisticRegression, newdata = test_data, type = "response")
logistic_pred <- ifelse(logistic_pred > 0.5, 1, 0)
predlasso <- predict(lasso_optLambda, newx=Mx[-train,], type="response")
predlasso <- predict(lasso_optLambda, newx=Mx, type="response")
predlasso
length(predlasso)
length(Tree_pred)
length(predlasso)
length(Tree_pred)
evaluation <- data.frame(
actual = test_data$Default,
logistic = logistic_pred,
tree = Tree_pred,
randomForest = randomForest_pred,
Lasso_Logistic = LassoLogistic_pred
)
evaluation <- data.frame(
actual = test_data$Default,
logistic = logistic_pred,
tree = Tree_pred,
randomForest = randomForest_pred,
Lasso_Logistic = LassoLogistic_pred
)
evaluation <- data.frame(
actual = test_data$Default,
logistic = logistic_pred,
tree = Tree_pred,
randomForest = randomForest_pred
)
rm(list = ls())
rm(list = ls())
