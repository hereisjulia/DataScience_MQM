library(ggplot2)
library(stringr)
library(RColorBrewer)
library(corrplot)
library(glmnet)
data <- read_csv("https://raw.githubusercontent.com/hereisjulia/DataScience_MQM/main/TermProject/Loan_default.csv")
data <- read_csv("https://raw.githubusercontent.com/hereisjulia/DataScience_MQM/main/TermProject/Loan_default.csv")
str(data)
summary(data)
length(data)
count(data)
data_drop <- c("LoanID")
data_clean <- data[, !names(data) %in% data_drop]
data_clean$HasMortgage <- ifelse(data_clean$HasMortgage == "Yes", 1, 0)
data_clean$HasDependents <- ifelse(data_clean$HasDependents == "Yes", 1, 0)
data_clean$HasCoSigner <- ifelse(data_clean$HasCoSigner == "Yes", 1, 0)
education_mapping <- c("High School"= 1, "Bachelor's" = 2, "Master's" = 3, "PhD" = 4)
data_clean$Education <- as.integer(factor(data_clean$Education, levels = names(education_mapping)))
EmploymentType_mapping <- c("Unemployed" = 0, "Self-employed" = 1,"Part-time" = 2, "Full-time" = 3)
data_clean$EmploymentType <- as.integer(factor(data_clean$EmploymentType, levels = names(EmploymentType_mapping)))
MaritalStatus_Mapping <- c("Single" = 0, "Divorced" = 1, "Married" = 2)
data_clean$MaritalStatus <- as.integer(factor(data_clean$MaritalStatus, levels = names(MaritalStatus_Mapping)))
LoanPurpose_mapping <- c("Other" = 0, "Auto" = 1, "Education" = 2, "Home" = 3, "Business" = 4)
data_clean$LoanPurpose <- as.integer(factor(data_clean$LoanPurpose, levels = names(LoanPurpose_mapping)))
length(data_clean)
data_clean
Corr <- cor(data_clean)
Corr
CorrplotColor <- brewer.pal(n = 8, name = "BrBG")
corrplot(Corr, method = "color",col = CorrplotColor, tl.col = "black")
#2. A pregnant patient came to the doctor's office concerned with the child. Although she is not gaining weight she is considering to keep smoking.
# In order to convince the mother to quit smoking, an appropriate regression model
# and interpretation was told the patient. Which of the following options would the medical provider argue to convince her?
#a) Based on
summary(model.a)$coef["smoke",]
#######
### R script for Online Test
### (Reminder: make sure all the files and
### scripts are in the working directory)
###
###
### Some auxiliary files to load
source("./Online_Test/DataAnalyticsFunctions.R")
#################################################################
installpkg("quantreg")
library(quantreg)
library(quantreg)
options(warn=-1)
#################################################################
### Load infant weight data
###
load("./Online_Test/WorkspaceOnlineTest1.RData")
###
### This creates a data frame called "DATA"
### This data set has 198377 observations and 15 variables
#################################################################
summary(DATA)
View(DATA)
#1. As discussed in class, when considering the birthweight,
# we are concern with low birthweight. Which of the following
# models is more suitable for that?
#a) model.a <- glm(weight ~ ., data = DATA)
model.a <- glm(weight ~ ., data = DATA)
## .means all the other variables
#b) model.b <- rq(weight ~ ., tau = 0.5, data = DATA)
model.b <- rq(weight ~ ., tau = 0.5, data = DATA)
#c) model.c <- rq(weight ~ ., tau = 0.1, data = DATA)
model.c <- rq(weight ~ ., tau = 0.1, data = DATA)
#d) model.d <- rq(weight ~ ., tau = 0.9, data = DATA)
model.d <- rq(weight ~ ., tau = 0.9, data = DATA)
#2. A pregnant patient came to the doctor's office concerned with the child. Although she is not gaining weight she is considering to keep smoking.
# In order to convince the mother to quit smoking, an appropriate regression model
# and interpretation was told the patient. Which of the following options would the medical provider argue to convince her?
#a) Based on
summary(model.a)$coef["smoke",]
# I am confident that by quitting smoking now you can increase the child's weight on average by 166g.
# This gain can help the child considerably.
#b) Looking at the tail is important to see the extreme impact smoking can have. Based on
summary(model.c)$coef["smoke",]
# I am confident that by quitting smoking now it can increase your child's weight by 176g which
# can  clinically help the cihld.
#c) Based on
summary(model.c)$coef["smoke",]
#3. To further provide guidance to patients, the medical provider decided to construct a prediction interval
# for the birthweight. Consider a patient which has the same attribute values as observation number 2922,
patient <- DATA[2922,]
patient
# Which interval below would be a valid 80% prediction interval?
#(a)
quantile(DATA$weight, probs=c(.1,.9))
patient
# Which interval below would be a valid 80% prediction interval?
#(a)
quantile(DATA$weight, probs=c(.1,.9))
#
#(b)
c( lower = predict(model.c, newdata=patient),
upper = predict(model.d, newdata=patient) )
#
#(c)
predict.lm(model.a, newdata=patient, interval="confidence", level = 0.80)
### Some auxiliary files to load
source("./Online_Test/DataAnalyticsFunctions.R")
#################################################################
### Load data data
churndata <- read.csv("./Online_Test/customerchurn.csv")
churndata$Churn<-factor(churndata$Churn)
churndata$tenure <- as.numeric(churndata$tenure)
#################################################################
### summarize the data
summary(churndata)
#a)
plot(factor(Churn) ~ factor(gender), data=churndata, col=c(8,2), ylab="Churn Rate", xlab="Gender")
#
#b)
plot(factor(Churn) ~ factor(SeniorCitizen), data=churndata, col=c(8,2), ylab="Churn Rate", xlab="Senior Citizen")
#
#c)
plot(factor(Churn) ~ MonthlyCharges, data=churndata, col=c(8,2), ylab="Churn Rate", xlab="Monthly charges")
#
#d)
plot(factor(Churn) ~ tenure, data=churndata, col=c(8,2), ylab="Churn Rate", xlab="Tenure (months)")
#
#b)
plot(factor(Churn) ~ factor(SeniorCitizen), data=churndata, col=c(8,2), ylab="Churn Rate", xlab="Senior Citizen")
#
#d)
plot(factor(Churn) ~ tenure, data=churndata, col=c(8,2), ylab="Churn Rate", xlab="Tenure (months)")
#a) Long term customers are more loyal to the company and therefore they are willing to pay more to stay with the company.
# This is exemplified by the positive correlation between the variables
cor(churndata$tenure,churndata$MonthlyCharges)
plot(MonthlyCharges~tenure, data=churndata, xlab='Monthly charges (dollars)', ylab='Tenure (months)', main='Churn')
#b) Long term customers have contracts which are older and hence more expensive as the technology industry reduces costs over time.
res_tenure.simple <- glm(MonthlyCharges~tenure, data=churndata)
summary(res_tenure.simple)
#c) The company seems to price only based on service feasures and does not
# seem to price discriminate based on customer's characteristics (as those coefficients
# are not statistically significant).
res_tenure <- glm(MonthlyCharges~.-customerID-Churn-TotalCharges, data=churndata)
summary(res_tenure)$coef[,c(1,4)]
#d) After accounting for a quadratic trend, and adding a new variable
# to the model, we see that the company also seems to price discriminate
# based on tenure (although with decreasing impact).
churndata$tenure.sq <- churndata$tenure^2
res_tenure.sq <- glm(MonthlyCharges~.-customerID-Churn-TotalCharges, data=churndata)
summary(res_tenure.sq)$coef[,c(1,4)]
#3. Based on the logistic regression model with all variables (except customerID) discussed in class,
result.logistic<-glm(Churn~.-customerID, data=churndata,family="binomial")
#among the customers 101 to 110, what is the highest probability of churn and how much is it?
predict(result.logistic,newdata=churndata[101:110,])
which.max(predict(result.logistic,newdata=churndata[101:110,],type="response"))
which.max(predict(result.logistic,newdata=churndata[101:110,],type="response"))
max(predict(result.logistic,newdata=churndata[101:110,],type="response"))
which.max(predict(result.logistic,newdata=churndata[101:110,],type="response"))
#4. Run the logistic regression model with all variables (except customerID) discussed in class
result.logistic <- glm(Churn~.-customerID, data=churndata, family="binomial")
# Next, we will use the model to classify using different thresholds
# on the predicted probability (not necessarily .5). We will use a function
# in FPR_TPR.R that computes the true positive rate and false positive rate.
# The code below plots several choices.
plot( c( 0, 1 ), c(0, 1), type="n", xlim=c(0,1), ylim=c(0,1), bty="n", xlab = "False positive rate", ylab="True positive rate")
lines(c(0,1),c(0,1), lty=2)
for ( val in seq(from = 0, to = 1, by = 0.05)  ){
values <- FPR_TPR( (result.logistic$fitted >= val) , result.logistic$y )
points( values$FPR , values$TPR )
}
lines(c(0,1),c(1,1), lty=2)
lines(c(1,1),c(0,1), lty=2)
lines(c(0.1,1),c(0,1), lty=2)
# Next, we will use the model to classify using different thresholds
# on the predicted probability (not necessarily .5). We will use a function
# in FPR_TPR.R that computes the true positive rate and false positive rate.
# The code below plots several choices.
plot( c( 0, 1 ), c(0, 1), type="n", xlim=c(0,1), ylim=c(0,1), bty="n", xlab = "False positive rate", ylab="True positive rate")
lines(c(0,1),c(0,1), lty=2)
for ( val in seq(from = 0, to = 1, by = 0.05)  ){
values <- FPR_TPR( (result.logistic$fitted >= val) , result.logistic$y )
points( values$FPR , values$TPR )
}
lines(c(0.1,0.1),c(0,1), lty=2)
lines(c(0.1,0.1),c(0,1), lty=2, col = "red")
# Next, we will use the model to classify using different thresholds
# on the predicted probability (not necessarily .5). We will use a function
# in FPR_TPR.R that computes the true positive rate and false positive rate.
# The code below plots several choices.
plot( c( 0, 1 ), c(0, 1), type="n", xlim=c(0,1), ylim=c(0,1), bty="n", xlab = "False positive rate", ylab="True positive rate")
lines(c(0,1),c(0,1), lty=2)
for ( val in seq(from = 0, to = 1, by = 0.05)  ){
values <- FPR_TPR( (result.logistic$fitted >= val) , result.logistic$y )
points( values$FPR , values$TPR )
}
lines(c(0.1,0.1),c(0,1), lty=2, col = "red")
#######################################################
### R script for Online Test:
###
### (Reminder: make sure all the files and
### scripts are in the working directory)
###
source("./Online_Test/DataAnalyticsFunctions.R")
load("./Online_Test/WorkspaceOnlineTest3part1.RData")
load("./Online_Test/WorkspaceOnlineTest3part1.RData")
source("./Final/DataAnalyticsFunctions.R")
######################################################
### The following two questions pertain to the churn problem
### discussed in class.
load("./Final/WorkspaceFinal-PartII-Churn.RData")
options(warn=-1)
summary(churndata)
library(glmnet)
#### Create a model with interactions of degree 3
Mx<- model.matrix(Churn ~ .^3, data=churndata)[,-1]
My<- churndata$Churn == "Yes"
### This defined the features we will use the matrix Mx (X) and
### the target My (Y)
num.features <- ncol(Mx)
num.n <- nrow(Mx)
num.churn <- sum(My)
w <- (num.churn/num.n)*(1-(num.churn/num.n))
#### For the binomial case, a theoretically valid choice is
lambda.theory <- sqrt(w*log(num.features/0.05)/num.n)
### next we call Lasso providing the
### features as matrix Mx
### the target as a vector My
### telling it is for logistic, family="binomial"
### and specifying lambda = lambda.theory
lassoTheory <- glmnet(Mx,My, family="binomial",lambda = lambda.theory)
### by calling the summary we see the list of object in sclassoTheory
summary(lassoTheory)
### lassoTheory$a0 gives you the intercept
### lassoTheory$beta gives you the coefficients of the 4179 variables.
### As expectd, many were set to zero.
### lassoTheory$lambda has the value of lambda stored.
### We can see the support of the selected coeffcients
### these are the indices
support(lassoTheory$beta,tr=0)
### the function supports returns the indices in the vector
### whose components'magnitude are above the value
### specified in tr. If no value of tr is specified, it sets tr=10^-6
### these are the labels
colnames(Mx)[support(lassoTheory$beta,tr=0)]
### there are in total
length(support(lassoTheory$beta,tr=0))
### variables
###
### If we omit the lambda, the function will solve for all values of lambda
### it takes a bit longer but not much longer at all.
lasso <- glmnet(Mx,My, family="binomial")
### Now that we can actually compute the Lasso for all values of lambda,
### the whole "path" of models can be evaluated by a OOS experiment
### we can attempt to use cross valiadation to actually pick lambda.
### the following command yields a cross validation procedure
### (note that this call will take some time)
lassoCV <- cv.glmnet(Mx,My, family="binomial")
### We can plot the fitting graph
### red dots are mean values and the bars are the uncertainty
par(mar=c(1.5,1.5,2,1.5))
par(mai=c(1.5,1.5,2,1.5))
### the following command takes some time.
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
### the following command takes some time.
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
### the following command takes some time.
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
line(x = lassoCV$lambda.min)
lines(x = lassoCV$lambda.min)
### the following command takes some time.
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
lines(x = lassoCV$lambda.min)
### the following command takes some time.
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
### We can plot the fitting graph
### red dots are mean values and the bars are the uncertainty
par(mar=c(1.5,1.5,2,1.5))
par(mai=c(1.5,1.5,2,1.5))
### the following command takes some time.
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
lassoCV$lambda.min
log(lassoCV$lambda.min)
lines(log(lassoCV$lambda.min), 0 )
lines(log(lassoCV$lambda.min), 0)
lines(c(log(lassoCV$lambda.min),log(lassoCV$lambda.min)), c(0, 2), lty=2)
lines(c(log(lassoCV$lambda.min),log(lassoCV$lambda.min)), c(0, 2), lty=2, col = "blue")
lines(c(log(lassoCV$lambda.1se),log(lassoCV$lambda.1se)), c(0, 2), lty=2, col = "green")
lassoTheory$lambda
lines(c(log(lassoTheory$lambda),log(lassoTheory$lambda)), c(0, 2), lty=2, col = "pink")
length(support(lassoCV$beta,tr=0))
length(support(lassoCV$beta))
length(support(lassoCV$lambda.min))
lassoMin <- glmnet(Mx,My, family="binomial",lambda = lassoCV$lambda.min)
length(support(lassoMin$beta))
### the following command takes some time.
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
lines(c(log(lassoCV$lambda.min),log(lassoCV$lambda.min)), c(0, 2), lty=2, col = "blue")
lines(c(log(lassoCV$lambda.1se),log(lassoCV$lambda.1se)), c(0, 2), lty=2, col = "green")
lines(c(log(lassoTheory$lambda),log(lassoTheory$lambda)), c(0, 2), lty=2, col = "pink")
length(support(lassoMin$beta, ty= 0))
length(support(lassoMin$beta, tr= 0))
lassoMin <- glmnet(Mx,My, family="binomial",lambda = lassoCV$lambda.min)
length(support(lassoMin$beta, tr= 0))
### if you need to look into the information in lassoCV
### you can see which are the fields by usinghttp://127.0.0.1:39427/graphics/07cce5a3-4ded-4cc5-97b7-66c6d7b9ad2e.png
attributes(lassoCV)
### Question 9. ####
### The code runs 10-fold CV for 4 methods:
### Lasso (theory and min choice of lambda);
### Post-Lasso (based on theory and min choices of lambda)
PL.OOS <- data.frame(PL.min=rep(NA,nfold),  PL.theory=rep(NA,nfold))
L.OOS <- data.frame(L.min=rep(NA,nfold),  L.theory=rep(NA,nfold))
features.min <- support(lasso$beta[,which.min(lassoCV$cvm)],tr=0)
which(lassoCV$lambda==lassoCV$lambda.min)
features.min <- support(lasso$beta[,which.min(lassoCV$cvm)],tr=0)
length(features.min)
features.theory <- support(lassoTheory$beta,tr=0)
length(features.theory)
data.min <- data.frame(Mx[,features.min],My)
data.theory <- data.frame(Mx[,features.theory],My)
for(k in 1:nfold){
train <- which(foldid!=k) # train on all but fold `k'
### This is the CV for the Post Lasso Estimates
rmin <- glm(My~., data=data.min, subset=train, family="binomial")
if ( length(features.theory) == 0){
rtheory <- glm(Churn~1, data=churndata, subset=train, family="binomial")
} else {rtheory <- glm(My~., data=data.theory, subset=train, family="binomial") }
predmin <- predict(rmin, newdata=data.min[-train,], type="response")
predtheory <- predict(rtheory, newdata=data.theory[-train,], type="response")
PL.OOS$PL.min[k] <- R2(y=My[-train], pred=predmin, family="binomial")
PL.OOS$PL.theory[k] <- R2(y=My[-train], pred=predtheory, family="binomial")
### This is the CV for the Lasso estimates
lassomin  <- glmnet(Mx[train,],My[train], family="binomial",lambda = lassoCV$lambda.min)
lassoTheory <- glmnet(Mx[train,],My[train], family="binomial",lambda = lambda.theory)
predlassomin <- predict(lassomin, newx=Mx[-train,], type="response")
predlassotheory <- predict(lassoTheory, newx=Mx[-train,], type="response")
L.OOS$L.min[k] <- R2(y=My[-train], pred=predlassomin, family="binomial")
L.OOS$L.theory[k] <- R2(y=My[-train], pred=predlassotheory, family="binomial")
print(paste("Iteration",k,"of",nfold,"completed"))
}
R2performance <- cbind(PL.OOS,L.OOS)
barplot(colMeans(R2performance), xpd=FALSE, ylim=c(0,.3) , xlab="Method", ylab = bquote( "10-Fold Cross Validation OOS" ~ R^2))
m.OOS <- as.matrix(R2performance)
rownames(m.OOS) <- c(1:nfold)
par(mar=c(1.5,1.5,1.5,3))
par(mai=c(1.5,1.5,1.5,3))
barplot(t(as.matrix(m.OOS)), beside=TRUE, ylim=c(0,.4) ,legend=TRUE, args.legend=c(xjust=0, yjust=1),
ylab= bquote( "Out of Sample " ~ R^2), xlab="Fold", names.arg = c(1:10))
library(ElemStatLearn)
### the german credit data was loaded in the workspace
### otherwise we should could call germancredit <- read.csv("germancredit.csv")
summary(germancredit)
source("./Final/DataAnalyticsFunctions.R")
load("./Final/WorkspaceFinal-PartII-German.RData")
options(warn=-1)
################################
####### k-NN
################################
installpkg("ElemStatLearn")
library(ElemStatLearn)
installpkg("class")
################################
####### k-NN
################################
installpkg("ElemStatLearn")
library(ElemStatLearn)
################################
####### k-NN
################################
installpkg("ElemStatLearn")
################################
####### k-NN
################################
installpkg("ElemStatLearn")
### the german credit data was loaded in the workspace
### otherwise we should could call germancredit <- read.csv("germancredit.csv")
summary(germancredit)
set.seed(1)
x <- model.matrix(Default~., data=germancredit)[,-1]
kfit <- lapply(1:200, function(k) kmeans(scale(x),k))
source("DataAnalyticsFunctions.R") # contsints the function kIC (you should have already loaded it)
source("./Final/DataAnalyticsFunctions.R") # contsints the function kIC (you should have already loaded it)
## Note that the function kIC selects k=# of clusters via Information Criteria
## there are two options: "A" for AIC (default) or "B" for BIC
kbic  <- sapply(kfit,kIC,"B")
plot(kbic, xlab="# of clusters (k)", ylab="Information Criterion", ylim=range(c(kbic)), type="l", lwd=2)
abline(v=which.min(kbic),col=4)
text(70,mean(kbic),"BIC")
kmin <- which.min(kbic)
# a) The radius of the bubble relates to the market size of the cluster, and the coordinate a given by variation explained.
radius <- sqrt(kfit[[kmin]]$size/pi)
symbols(kfit[[kmin]]$withinss, (kfit[[kmin]]$withinss/kfit[[kmin]]$totss),circles=radius, xlab = "Variation in the cluster", ylab="% of total variation", bg = col, inches = .5)
text((kfit[[kmin]]$withinss), (kfit[[kmin]]$withinss/kfit[[kmin]]$totss), names=1:kfit[[kmin]]$size)
# b) The radius of the bubble relates to the market size of the cluster, and the coordinate a given by explained variation.
radius <- sqrt(kfit[[kmin]]$size/pi)
symbols((kfit[[kmin]]$withinss/kfit[[kmin]]$size), (kfit[[kmin]]$withinss/kfit[[kmin]]$totss),circles=radius, xlab = "Average of variation per observation in the cluster", ylab="% of total variation", bg = col, inches = .5)
text((kfit[[kmin]]$withinss/kfit[[kmin]]$size), (kfit[[kmin]]$withinss/kfit[[kmin]]$totss), names=1:kfit[[kmin]]$size)
# c) The radius of the bubble relates to default rates, and the coordinate a given by explained variation.
cluster.default <- tapply(germancredit$Default, kfit[[kmin]]$cluster,mean)
radius <- sqrt(cluster.default)
symbols((kfit[[kmin]]$withinss/kfit[[kmin]]$size), (kfit[[kmin]]$withinss/kfit[[kmin]]$totss),circles=radius, xlab = "Average of variation per observation in the cluster", ylab="% of total variation", bg = col, inches = .5)
text((kfit[[kmin]]$withinss/kfit[[kmin]]$size), (kfit[[kmin]]$withinss/kfit[[kmin]]$totss), names=1:kfit[[kmin]]$size)
# d) The radius of the bubble relates to default rates, and the coordinate a given by segment size and average loan within cluster.
cluster.amount <- tapply(germancredit$amount, kfit[[kmin]]$cluster,mean)
cluster.default <- tapply(germancredit$Default, kfit[[kmin]]$cluster,mean)
radius <- sqrt(cluster.default)
symbols((cluster.amount), (kfit[[kmin]]$size),circles=radius, xlab = "Average loan amount within cluster", ylab="Segment size", bg = col, inches = .5)
text((cluster.amount), (kfit[[kmin]]$size), names=1:kfit[[kmin]]$size)
#11. The marketing consultant wants to uncover "latent variables" of customers.
# For that he intends to use PCA and study the "top variables" in the first principal component.
#
installpkg("plfm")
library(plfm)
library(plfm)
x <- model.matrix(Default~., data=germancredit)[,-1]
## the command above return the matrix X associated with the model
## Default~. under the dataframe "germancredit (recall that Default is the variable Yes/No for the loan)
## the last part "[,-1]" removes the intercept from the matrix which was automatically created
feature.labels <- colnames(x)
pca.features <- prcomp(x, scale=TRUE)
#pich the first principal component
loadings <- pca.features$rotation[,1]
## Pick the incorrect statement:
# a) The first principal component seems explain more than any other principal component.
plot(pca.features,main="PCA: Variance Explained by Factors")
mtext(side=1, "Factors",  line=1, font=2)
#
# b) The most important variable in the first factor is
which.max(loadings)
max(loadings)
#
# c) The first component uses a combination of all attributes (even if with small coefficient).
loadings
max(loadings)
#
# b) The most important variable in the first factor is
which.max(loadings)
max(loadings)
#
# c) The first component uses a combination of all attributes (even if with small coefficient).
loadings
### the german credit data was loaded in the workspace
### otherwise we should could call germancredit <- read.csv("germancredit.csv")
summary(germancredit)
str(germancredit)
#
# c) The first component uses a combination of all attributes (even if with small coefficient).
length(loadings)
#
# c) The first component uses a combination of all attributes (even if with small coefficient).
loadings
names(germancredit)
#
# c) The first component uses a combination of all attributes (even if with small coefficient).
loadings
#
# d) Looking at the loadings ordered by magnitude
loadings[order(abs(loadings), decreasing=TRUE)[1:length(loadings)]]
pixels <- 40*20
layers_a <- c(16,16,16,1)
layers_b <- c(32,16, 8,1)
layers_c <- c(16,32, 8,1)
layers_d <- c(16,32, 4,1)
12560+272+272
12560+272+272-13121
parameter$a <- pixels+1 + (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
layers <- c(16,16,16,1)
parameter$a <- pixels+1 + (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
parameter <- list()
parameter$a <- pixels+1 + (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
pixels <- 40*20
parameter <- list()
layers <- c(16,16,16,1)
parameter$a <- pixels+1 + (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
layers <- c(32,16, 8,1)
parameter$b <- pixels+1 + (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
layers <- c(16,32, 8,1)
parameter$c <- pixels+1 + (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
layers <- c(16,32, 4,1)
parameter$d <- pixels+1 + (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
parameter
parameter-801
parameter <- matrix()
layers <- c(16,16,16,1)
parameter$a <- pixels+1 + (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
layers <- c(32,16, 8,1)
parameter$b <- pixels+1 + (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
layers <- c(16,32, 8,1)
parameter$c <- pixels+1 + (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
layers <- c(16,32, 4,1)
parameter$d <- pixels+1 + (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
parameter-801
parameter
pixels <- 40*20
parameter <- matrix()
layers <- c(16,16,16,1)
parameter$a <-  (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
layers <- c(32,16, 8,1)
parameter$b <- (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
layers <- c(16,32, 8,1)
parameter$c <- (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
layers <- c(16,32, 4,1)
parameter$d <- (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
parameter
layers <- c(16, 8, 8,1)
pixels <- 40*20
parameter <- matrix()
layers <- c(16,16,16,1)
parameter$a <-  (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
layers <- c(32,32,32,1)
parameter$b <- (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
layers <- c(64,32,32,1)
parameter$c <- (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
layers <- c(16, 8, 8,1)
parameter$d <- (pixels+1)*layers[1] + (layers[1]+1)*layers[2] + (layers[2]+1)*layers[3] + (layers[3]+1)*layers[4]
parameter
