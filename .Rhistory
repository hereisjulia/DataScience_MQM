rf_model$pred
rf_model$modelType
rf_model$modelInfo
rf_model$method
rf_model$call
View(election_data_train)
drop2 <- c("County","State", "Region", "FIPS","Obama_margin","Obama_wins", "TotalVote", "Clinton", "Obama")
election_data_train <- election_data_train[,!names(election_data_train) %in% drop2]
ctrl <- trainControl(
method = "cv",    # Cross-validation method
number = 5,      # Number of folds (e.g., 10-fold CV)
verboseIter = TRUE
)
set.seed(123)  # Set a random seed for reproducibility
rf_model <- train(
x = election_data_train,          # Predictor variables
y = election_data_train$Obama_margin_percent,          # Target variable
method = "rf",  # Random Forest method
trControl = ctrl
)
ggplot(rf_model$resample)+
geom_col(aes(x = Resample, y = Rsquared), fill = "#003087")+
geom_text(aes(x = Resample, y = Rsquared, label = round(Rsquared,5)))+
#coord_cartesian(ylim = c(0.9975, 1))+
labs(title = "5-Fold Cross Validation", x = "", y = "Out of Sample R2")
rf_model <- train(
x = election_data_train[,!names(election_data_train)=="Obama_margin_percent"],          # Predictor variables
y = election_data_train$Obama_margin_percent,          # Target variable
method = "rf",  # Random Forest method
trControl = ctrl
)
ggplot(rf_model$resample)+
geom_col(aes(x = Resample, y = Rsquared), fill = "#003087")+
geom_text(aes(x = Resample, y = Rsquared, label = round(Rsquared,5)))+
#coord_cartesian(ylim = c(0.9975, 1))+
labs(title = "5-Fold Cross Validation", x = "", y = "Out of Sample R2")
test_data <- test_data_OOS[,!names(test_data_OOS) %in% drop2]
train_data <- train_data_OOS[,!names(train_data_OOS) %in% drop2]
rf_model$resample
predict(rf_model, newdata = election_data_test)
test_data <- test_data_OOS[,!names(test_data_OOS) %in% drop2]
train_data <- train_data_OOS[,!names(train_data_OOS) %in% drop2]
#LinearModel
linearModel <- glm(Obama_margin_percent ~., data = train_data)
summary(linearModel)
LM_inSample <- as.numeric(predict(linearModel, newdata = train_data))
LM_pred <- as.numeric(predict(linearModel, newdata = test_data))
LM_perform <- cbind(test_data, LM_pred) %>% mutate(LMdiff = LM_pred-Obama_margin_percent)
#CART
train_data$ElectionDate <- as.numeric(train_data$ElectionDate)
test_data$ElectionDate <- as.numeric(test_data$ElectionDate)
tree <- ctree(Obama_margin_percent ~., data = train_data)
tree_inSample <- predict(tree, newdata = train_data)
tree_pred <- predict(tree, newdata = test_data)
tree_perform <- cbind(test_data, tree_pred)
length(tree_perform)
names(tree_perform)[36] <- "tree_pred"
tree_perform <- tree_perform %>% mutate(Treediff = tree_pred - Obama_margin_percent)
#RandomForest
train_data <- train_data_OOS[,!names(train_data_OOS) %in% drop1]
#RandomForest
train_data <- train_data_OOS[,!names(train_data_OOS) %in% drop2]
test_data <- test_data_OOS[,!names(test_data_OOS) %in% drop2]
randomForest <- randomForest(Obama_margin_percent ~ ., data = train_data, ntree = 100, mtry = 8, importance = TRUE)
randomForest_inSample <- predict(randomForest, newdata = train_data)
randomForest_pred <- predict(randomForest, newdata = test_data)
randomForest_perform <- cbind(test_data, randomForest_pred)
randomForest_perform <- randomForest_perform %>% mutate(randomForestdiff = randomForest_pred - Obama_margin_percent)
R2_matrix <- data.frame(
"Model" = c("Linear_Model", "CART", "Random_Forest"),
"inSample_R2" = c(
R2(train_data$Obama_margin_percent, LM_inSample, family = "gaussian"),
R2(train_data$Obama_margin_percent, tree_inSample, family = "gaussian"),
R2(train_data$Obama_margin_percent, randomForest_inSample, family = "gaussian")
),
"OOS_R2" = c(
R2(LM_perform$Obama_margin_percent, LM_perform$LM_pred, family = "gaussian"),
R2(tree_perform$Obama_margin_percent, tree_perform$tree_pred, family = "gaussian"),
R2(randomForest_perform$Obama_margin_percent, randomForest_perform$randomForest_pred, family = "gaussian")
))
R2_matrix
ggplot(rf_model$resample)+
geom_col(aes(x = Resample, y = Rsquared), fill = "#003087")+
geom_text(aes(x = Resample, y = Rsquared, label = round(Rsquared,5)))+
#coord_cartesian(ylim = c(0.9975, 1))+
labs(title = "5-Fold Cross Validation", x = "", y = "Out of Sample R2")
ggplot(rf_model$resample)+
geom_col(aes(x = Resample, y = Rsquared), fill = "#003087")+
geom_text(aes(x = Resample, y = Rsquared, label = round(Rsquared,5)))+
coord_cartesian(ylim = c(0.4, 1))+
labs(title = "5-Fold Cross Validation", x = "", y = "Out of Sample R2")
ggplot(rf_model$resample)+
geom_col(aes(x = Resample, y = Rsquared), fill = "#003087")+
geom_text(aes(x = Resample, y = Rsquared + 0.05, label = round(Rsquared,5)))+
coord_cartesian(ylim = c(0.4, 1))+
labs(title = "5-Fold Cross Validation", x = "", y = "Out of Sample R2")
ggplot(rf_model$resample)+
geom_col(aes(x = Resample, y = Rsquared), fill = "#003087")+
geom_text(aes(x = Resample, y = Rsquared + 0.03, label = round(Rsquared,5)))+
coord_cartesian(ylim = c(0.4, 1))+
labs(title = "5-Fold Cross Validation", x = "", y = "Out of Sample R2")
View(election_data_test)
election_data_test$Obama_margin_percent <- predict(rf_model, newdata = election_data_test)
tf_model1 <- randomForest(Obama_margin_percent ~ ., data = train_data, ntree = 100, mtry = 34, importance = TRUE)
predict(rf_model1, newdata = election_data_test)
rf_model1 <- randomForest(Obama_margin_percent ~ ., data = train_data, ntree = 100, mtry = 34, importance = TRUE)
predict(rf_model1, newdata = election_data_test)
View(election_data_test)
election_data_test$Obama_margin_percent1 <- predict(rf_model1, newdata = election_data_test)
R2_matrix <- data.frame(
"Model" = c("Linear_Model", "CART", "Random_Forest"),
"inSample_R2" = c(
R2(train_data$Obama_margin_percent, LM_inSample, family = "gaussian"),
R2(train_data$Obama_margin_percent, tree_inSample, family = "gaussian"),
R2(train_data$Obama_margin_percent, randomForest_inSample, family = "gaussian")
),
"OOS_R2" = c(
R2(LM_perform$Obama_margin_percent, LM_perform$LM_pred, family = "gaussian"),
R2(tree_perform$Obama_margin_percent, tree_perform$tree_pred, family = "gaussian"),
R2(randomForest_perform$Obama_margin_percent, predict(rf_model1, newdata = test_data), family = "gaussian")
))
R2_matrix
randomForest <- randomForest(Obama_margin_percent ~ ., data = train_data, ntree = 100, mtry = 34, importance = TRUE)
election_data_test$Obama_margin_percent <- predict(rf_model, newdata = election_data_test)
library(readr)
library(tidyverse)
library(ggplot2)
library(stringr)
library(caret)
library(glmnet)
library(randomForest)
library(dplyr)
library(cluster)
library(party)
election_data <- read_csv("./Case3/ElectionDataAlone.csv")
election_data <- read_csv("https://raw.githubusercontent.com/hereisjulia/DataScience_MQM/main/Case3/ElectionDataAlone.csv")
impute_data <- function(vec, mn) {
ifelse(is.na(vec), mn, vec)
}
data_mean <- sapply(election_data[,10:41],mean, na.rm=TRUE)
(data_mean)
for(i in 10:41) {
election_data[,i]<-impute_data(election_data[,i],data_mean[i-9])
}
for(i in 10:41) {
election_data[,i]<-impute_data(election_data[,i],data_mean[i-9])
}
summary(election_data)
election_data[,i] <- impute_data(election_data[,i],data_mean[i-9])
election_data[,i] <- impute_data(election_data[,i],data_mean[i-9])
summary(rf_model)
election_data <- read_csv("https://raw.githubusercontent.com/hereisjulia/DataScience_MQM/main/Case3/ElectionDataAlone.csv")
impute_data <- function(vec, mn) {
ifelse(is.na(vec), mn, vec)
}
data_mean <- sapply(election_data[,10:41],mean, na.rm=TRUE)
(data_mean)
for(i in 10:41) {
election_data[,i] <- impute_data(election_data[,i],data_mean[i-9])
}
#Factor conversion
election_data$County <- as.factor(election_data$County)
election_data$State <- as.factor(election_data$State)
election_data$FIPS <- as.factor(election_data$FIPS)
election_data$Region <- as.factor(election_data$Region)
election_data$ElectionType <- as.factor(election_data$ElectionType)
#Split the original file
election_data$ElectionDate <- as.Date(election_data$ElectionDate, format="%m/%d/%Y")
election_data_train <- election_data[election_data$ElectionDate < as.Date("2/19/2008", format="%m/%d/%Y"), ]
election_data_test <- election_data[election_data$ElectionDate >= as.Date("2/19/2008", format="%m/%d/%Y"), ]
#Add margins
election_data_train$Obama_margin <- election_data_train$Obama - election_data_train$Clinton
election_data_train$Obama_margin_percent <- 100*election_data_train$Obama_margin/election_data_train$TotalVote
election_data_train$Obama_wins <- ifelse(election_data_train$Obama_margin >0, 1,0)
#Add margins
election_data_train$Obama_margin <- election_data_train$Obama - election_data_train$Clinton
election_data_train$Obama_margin_percent <- 100*election_data_train$Obama_margin/election_data_train$TotalVote
election_data_train$Obama_wins <- ifelse(election_data_train$Obama_margin >0, 1,0)
##plot1
us_map <- map_data("state")
state_abbr_mapping <- data.frame(
State = tolower(c("Alabama", "Arizona", "Arkansas", "California", "Colorado", "Connecticut", "Delaware", "Florida", "Georgia", "Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada", "New Hampshire", "New Jersey", "New Mexico", "New York", "North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", "South Carolina", "South Dakota", "Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington", "West Virginia", "Wisconsin", "Wyoming")),
Abbreviation = c("AL", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA", "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY")
)
us_map$State <- state_abbr_mapping$Abbreviation[match(us_map$region, state_abbr_mapping$State)]
state_map <- merge(us_map, election_data_train, by.x = "State", by.y = "State", all.x = TRUE)
state_map <- merge(us_map, election_data_train, by.x = "State", by.y = "State", all.x = TRUE)
ggplot(state_map)+
geom_map(aes(fill = Obama_margin_percent, x =  long, y = lat, map_id = region), map = us_map, colour="white", size = 0.5)+
labs(title = "Obama's Margin in States", fill = "Margin(%)") +
theme_void()
##plot2
a<-election_data_train %>% group_by(Region) %>% summarize(ObamaVotes = mean(Obama/TotalVote)*100, PovertyRate = mean(Poverty))%>%
pivot_longer(-c(1), names_to = "type", values_to = "Percent" )
ggplot(data = a)+
geom_col(aes(x = Region, y = Percent, fill = type), position = position_dodge(width = 0.8))+
geom_text(aes(x = Region, y = Percent, label = round(Percent,2)))+
labs(title = "Votes for Obama (%) vs Poverty Rate (%)")
#Split the training data for OOS
set.seed(123456)
train_indices <- sample(1:nrow(election_data_train), 0.8 * nrow(election_data_train))
train_data_OOS <- election_data_train[train_indices, ]
test_data_OOS <- election_data_train[-train_indices, ]
#Column deletion
drop1 <- c("County", "State", "Region", "FIPS")
drop2 <- c("County","State", "Region", "FIPS","Obama_margin","Obama_wins", "TotalVote", "Clinton", "Obama")
test_data <- test_data_OOS[,!names(test_data_OOS) %in% drop2]
train_data <- train_data_OOS[,!names(train_data_OOS) %in% drop2]
election_data_train <- election_data_train[,!names(election_data_train) %in% drop2]
#Column deletion
drop1 <- c("County", "State", "Region", "FIPS")
drop2 <- c("County","State", "Region", "FIPS","Obama_margin","Obama_wins", "TotalVote", "Clinton", "Obama")
test_data <- test_data_OOS[,!names(test_data_OOS) %in% drop2]
train_data <- train_data_OOS[,!names(train_data_OOS) %in% drop2]
election_data_train <- election_data_train[,!names(election_data_train) %in% drop2]
## build models
```{r}
#LinearModel
linearModel <- glm(Obama_margin_percent ~., data = train_data)
summary(linearModel)
LM_inSample <- as.numeric(predict(linearModel, newdata = train_data))
LM_pred <- as.numeric(predict(linearModel, newdata = test_data))
LM_perform <- cbind(test_data, LM_pred) %>% mutate(LMdiff = LM_pred-Obama_margin_percent)
#CART
train_data$ElectionDate <- as.numeric(train_data$ElectionDate)
test_data$ElectionDate <- as.numeric(test_data$ElectionDate)
tree <- ctree(Obama_margin_percent ~., data = train_data)
tree_inSample <- predict(tree, newdata = train_data)
tree_pred <- predict(tree, newdata = test_data)
tree_perform <- cbind(test_data, tree_pred)
names(tree_perform)[36] <- "tree_pred"
tree_perform <- tree_perform %>% mutate(Treediff = tree_pred - Obama_margin_percent)
#RandomForest
train_data <- train_data_OOS[,!names(train_data_OOS) %in% drop2]
test_data <- test_data_OOS[,!names(test_data_OOS) %in% drop2]
randomForest <- randomForest(Obama_margin_percent ~ ., data = train_data, ntree = 100, mtry = 34, importance = TRUE)
randomForest_inSample <- predict(randomForest, newdata = train_data)
randomForest <- randomForest(Obama_margin_percent ~ ., data = train_data, ntree = 100, mtry = 34, importance = TRUE)
election_data <- na.omit(election_data)
#Factor conversion
election_data$County <- as.factor(election_data$County)
election_data$State <- as.factor(election_data$State)
election_data$FIPS <- as.factor(election_data$FIPS)
election_data$Region <- as.factor(election_data$Region)
election_data$ElectionType <- as.factor(election_data$ElectionType)
#Split the original file
election_data$ElectionDate <- as.Date(election_data$ElectionDate, format="%m/%d/%Y")
election_data_train <- election_data[election_data$ElectionDate < as.Date("2/19/2008", format="%m/%d/%Y"), ]
election_data_test <- election_data[election_data$ElectionDate >= as.Date("2/19/2008", format="%m/%d/%Y"), ]
#Add margins
election_data_train$Obama_margin <- election_data_train$Obama - election_data_train$Clinton
election_data <- read_csv("https://raw.githubusercontent.com/hereisjulia/DataScience_MQM/main/Case3/ElectionDataAlone.csv")
election_data <- na.omit(election_data)
#Factor conversion
election_data$County <- as.factor(election_data$County)
election_data$State <- as.factor(election_data$State)
election_data$FIPS <- as.factor(election_data$FIPS)
election_data$Region <- as.factor(election_data$Region)
election_data$ElectionType <- as.factor(election_data$ElectionType)
#Split the original file
election_data$ElectionDate <- as.Date(election_data$ElectionDate, format="%m/%d/%Y")
election_data_train <- election_data[election_data$ElectionDate < as.Date("2/19/2008", format="%m/%d/%Y"), ]
election_data_test <- election_data[election_data$ElectionDate >= as.Date("2/19/2008", format="%m/%d/%Y"), ]
#Add margins
election_data_train$Obama_margin <- election_data_train$Obama - election_data_train$Clinton
election_data_train$Obama_margin_percent <- 100*election_data_train$Obama_margin/election_data_train$TotalVote
election_data_train$Obama_wins <- ifelse(election_data_train$Obama_margin >0, 1,0)
#Add margins
election_data_train$Obama_margin <- election_data_train$Obama - election_data_train$Clinton
election_data_train$Obama_margin_percent <- 100*election_data_train$Obama_margin/election_data_train$TotalVote
election_data_train$Obama_wins <- ifelse(election_data_train$Obama_margin >0, 1,0)
##plot1
us_map <- map_data("state")
state_abbr_mapping <- data.frame(
State = tolower(c("Alabama", "Arizona", "Arkansas", "California", "Colorado", "Connecticut", "Delaware", "Florida", "Georgia", "Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada", "New Hampshire", "New Jersey", "New Mexico", "New York", "North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", "South Carolina", "South Dakota", "Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington", "West Virginia", "Wisconsin", "Wyoming")),
Abbreviation = c("AL", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA", "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY")
)
us_map$State <- state_abbr_mapping$Abbreviation[match(us_map$region, state_abbr_mapping$State)]
state_map <- merge(us_map, election_data_train, by.x = "State", by.y = "State", all.x = TRUE)
state_map <- merge(us_map, election_data_train, by.x = "State", by.y = "State", all.x = TRUE)
ggplot(state_map)+
geom_map(aes(fill = Obama_margin_percent, x =  long, y = lat, map_id = region), map = us_map, colour="white", size = 0.5)+
labs(title = "Obama's Margin in States", fill = "Margin(%)") +
theme_void()
##plot2
a<-election_data_train %>% group_by(Region) %>% summarize(ObamaVotes = mean(Obama/TotalVote)*100, PovertyRate = mean(Poverty))%>%
pivot_longer(-c(1), names_to = "type", values_to = "Percent" )
ggplot(data = a)+
geom_col(aes(x = Region, y = Percent, fill = type), position = position_dodge(width = 0.8))+
geom_text(aes(x = Region, y = Percent, label = round(Percent,2)))+
labs(title = "Votes for Obama (%) vs Poverty Rate (%)")
#Split the training data for OOS
set.seed(123456)
train_indices <- sample(1:nrow(election_data_train), 0.8 * nrow(election_data_train))
train_data_OOS <- election_data_train[train_indices, ]
test_data_OOS <- election_data_train[-train_indices, ]
#Column deletion
drop1 <- c("County", "State", "Region", "FIPS")
drop2 <- c("County","State", "Region", "FIPS","Obama_margin","Obama_wins", "TotalVote", "Clinton", "Obama")
test_data <- test_data_OOS[,!names(test_data_OOS) %in% drop2]
train_data <- train_data_OOS[,!names(train_data_OOS) %in% drop2]
election_data_train <- election_data_train[,!names(election_data_train) %in% drop2]
#Column deletion
drop1 <- c("County", "State", "Region", "FIPS")
drop2 <- c("County","State", "Region", "FIPS","Obama_margin","Obama_wins", "TotalVote", "Clinton", "Obama")
test_data <- test_data_OOS[,!names(test_data_OOS) %in% drop2]
train_data <- train_data_OOS[,!names(train_data_OOS) %in% drop2]
election_data_train <- election_data_train[,!names(election_data_train) %in% drop2]
#LinearModel
linearModel <- glm(Obama_margin_percent ~., data = train_data)
LM_inSample <- as.numeric(predict(linearModel, newdata = train_data))
LM_pred <- as.numeric(predict(linearModel, newdata = test_data))
LM_perform <- cbind(test_data, LM_pred) %>% mutate(LMdiff = LM_pred-Obama_margin_percent)
#CART
train_data$ElectionDate <- as.numeric(train_data$ElectionDate)
test_data$ElectionDate <- as.numeric(test_data$ElectionDate)
tree <- ctree(Obama_margin_percent ~., data = train_data)
tree_inSample <- predict(tree, newdata = train_data)
tree_pred <- predict(tree, newdata = test_data)
tree_perform <- cbind(test_data, tree_pred)
names(tree_perform)[36] <- "tree_pred"
tree_perform <- tree_perform %>% mutate(Treediff = tree_pred - Obama_margin_percent)
#RandomForest
train_data <- train_data_OOS[,!names(train_data_OOS) %in% drop2]
test_data <- test_data_OOS[,!names(test_data_OOS) %in% drop2]
randomForest <- randomForest(Obama_margin_percent ~ ., data = train_data, ntree = 100, mtry = 34, importance = TRUE)
randomForest <- randomForest(Obama_margin_percent ~ ., data = train_data, ntree = 100, mtry = 34, importance = TRUE)
randomForest_inSample <- predict(randomForest, newdata = train_data)
randomForest_pred <- predict(randomForest, newdata = test_data)
randomForest_perform <- cbind(test_data, randomForest_pred)
randomForest_perform <- randomForest_perform %>% mutate(randomForestdiff = randomForest_pred - Obama_margin_percent)
R2_matrix <- data.frame(
"Model" = c("Linear_Model", "CART", "Random_Forest"),
"inSample_R2" = c(
R2(train_data$Obama_margin_percent, LM_inSample, family = "gaussian"),
R2(train_data$Obama_margin_percent, tree_inSample, family = "gaussian"),
R2(train_data$Obama_margin_percent, randomForest_inSample, family = "gaussian")
),
"OOS_R2" = c(
R2(LM_perform$Obama_margin_percent, LM_perform$LM_pred, family = "gaussian"),
R2(tree_perform$Obama_margin_percent, tree_perform$tree_pred, family = "gaussian"),
R2(randomForest_perform$Obama_margin_percent, randomForest_perform$randomForest_pred, family = "gaussian")
))
if(fam=="binomial"){
if(is.factor(y)){ y <- as.numeric(y)>1 }
}
## This file contains several functions used in the course
##
## These codes are used for installing packages
## function for installing needed packages
installpkg <- function(x){
if(x %in% rownames(installed.packages())==FALSE) {
if(x %in% rownames(available.packages())==FALSE) {
paste(x,"is not a valid package - please check again...")
} else {
install.packages(x)
}
} else {
paste(x,"package already installed...")
}
}
###################
###################
### Used on Class 3 and Online Test 3
### FPR_TPR
FPR_TPR <- function(prediction, actual){
TP <- sum((prediction)*(actual))
FP <- sum((prediction)*(!actual))
FN <- sum((!prediction)*(actual))
TN <- sum((!prediction)*(!actual))
result <- data.frame( FPR = FP / (FP + TN), TPR = TP / (TP + FN), ACC = (TP+TN)/(TP+TN+FP+FN) )
return (result)
}
BinaryAccuracy <- function(prediction, actual){
TP <- sum((prediction)*(actual))
FP <- sum((prediction)*(!actual))
FN <- sum((!prediction)*(actual))
TN <- sum((!prediction)*(!actual))
result <-  (TP+TN)/(TP+TN+FP+FN)
return (result)
}
#################################################
### Functions used in Class 4 and Online Test 4
#
## deviance calculations
## pred must be probabilities (0<pred<1) for binomial
deviance <- function(y, pred, family=c("gaussian","binomial")){
family <- match.arg(family)
if(family=="gaussian"){
return( sum( (y-pred)^2 ) )
}else{
if(is.factor(y)) y <- as.numeric(y)>1
return( -2*sum( y*log(pred) + (1-y)*log(1-pred) ) )
}
}
devianceQR <- function(y, pred, tau){
return( sum(  tau*max(0, y-pred ) + (1-tau)*max(0, pred-y ) ) )
}
## get null devaince too, and return R2
R2 <- function(y, pred, family=c("gaussian","binomial")){
fam <- match.arg(family)
if(fam=="binomial"){
if(is.factor(y)){ y <- as.numeric(y)>1 }
}
dev <- deviance(y, pred, family=fam)
dev0 <- deviance(y, mean(y), family=fam)
return(1-dev/dev0)
}
### Returns the indices for which |x[i]| > tr
support<- function(x, tr = 10e-6) {
m<- rep(0, length(x))
for (i in 1:length(x)) if( abs(x[i])> tr ) m[i]<- i
m <- m[m>0]
m
}
### Penalty choice for Quantile Regression
lambda.BC<- function(X, R = 1000, tau = 0.5, c = 1, alpha = .05){
n <- nrow(X)
norm2n<-function(z){sqrt(mean(z^2))}
sigs <- apply(X,2,norm2n)
U <- matrix(runif(n * R),n)
R <- (t(X) %*% (tau - (U < tau)))/(sigs*sqrt(tau*(1-tau)))
r <- apply(abs(R),2,max)
c * quantile(r, 1 - alpha) * sqrt(tau*(1-tau))*c(1,sigs)
}
### Used in Class 5, Class 9 and Online Test 5
## Selects the Number of Clusters via an Information Criteria
## get AIC (option "A"), BIC (option "B"), HDIC (option "C") for the output of kmeans
kIC <- function(fit, rule=c("A","B","C")){
df <- length(fit$centers) # K*dim
n <- sum(fit$size)
D <- fit$tot.withinss # deviance
rule=match.arg(rule)
if(rule=="A")
#return(D + 2*df*n/max(1,n-df-1))
return(D + 2*df)
else if(rule=="B")
return(D + log(n)*df)
else
return(D +  sqrt( n * log(df) )*df)
}
R2_matrix <- data.frame(
"Model" = c("Linear_Model", "CART", "Random_Forest"),
"inSample_R2" = c(
R2(train_data$Obama_margin_percent, LM_inSample, family = "gaussian"),
R2(train_data$Obama_margin_percent, tree_inSample, family = "gaussian"),
R2(train_data$Obama_margin_percent, randomForest_inSample, family = "gaussian")
),
"OOS_R2" = c(
R2(LM_perform$Obama_margin_percent, LM_perform$LM_pred, family = "gaussian"),
R2(tree_perform$Obama_margin_percent, tree_perform$tree_pred, family = "gaussian"),
R2(randomForest_perform$Obama_margin_percent, randomForest_perform$randomForest_pred, family = "gaussian")
))
R2_matrix
library(caret)
ctrl <- trainControl(
method = "cv",    # Cross-validation method
number = 5,      # Number of folds (e.g., 10-fold CV)
verboseIter = TRUE
)
set.seed(123)  # Set a random seed for reproducibility
rf_model <- train(
x = election_data_train[,!names(election_data_train)=="Obama_margin_percent"],          # Predictor variables
y = election_data_train$Obama_margin_percent,          # Target variable
method = "rf",  # Random Forest method
trControl = ctrl
)
ggplot(rf_model$resample)+
geom_col(aes(x = Resample, y = Rsquared), fill = "#003087")+
geom_text(aes(x = Resample, y = Rsquared + 0.03, label = round(Rsquared,5)))+
coord_cartesian(ylim = c(0.4, 1))+
labs(title = "5-Fold Cross Validation", x = "", y = "Out of Sample R2")
summary(rf_model)
subset3 <- select(election_data, -County, -State, - Region, -FIPS, -ElectionDate, -ElectionType, -TotalVote, -Obama, -Clinton)
df3 <- scale(subset3)
# Number of clusters to consider
max_k <- 15
wss <- numeric(max_k)
for (k in 1:max_k) {
kmeans_result <- kmeans(df3, centers = k, nstart = 25)
wss[k] <- kmeans_result$tot.withinss
}
for (k in 1:max_k) {
kmeans_result <- kmeans(df3, centers = k, nstart = 25)
wss[k] <- kmeans_result$tot.withinss
}
# Plot the total within sum of squares vs. number of clusters
tibble(k = 1:max_k, wss = wss) %>%
ggplot(aes(x = k, y = wss)) +
geom_line() +
geom_point() +
ggtitle("Elbow Method for Optimal k") +
xlab("Number of clusters (k)") +
ylab("Total Within Sum of Squares")
#KMeans
set.seed(123)
km.res <- kmeans(df3, 4, nstart = 25)
print(km.res)
dd <- cbind(election_data, cluster = km.res$cluster)
head(dd)
#Heatmap
cluster_centers <- km.res$centers
heatmap(as.matrix(cluster_centers),
main="Cluster Centers",
scale="row",
Colv=NA)
heatmap(as.matrix(cluster_centers), labRow = row_labels, labCol = column_labels,
cexRow = 0.8, cexCol = 0.8, srtCol = 45)
heatmap(as.matrix(cluster_centers),
main="Cluster Centers",
scale="row",
Colv=NA)
