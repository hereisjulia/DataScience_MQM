### Provide (a simple) evidence that supports your view based on data.
###
### Yes! It sonds very likely that some of these variables we
### will have some explanatory value. Simple measure: correlation.
### It seems reasonable that knowing if it is a "Boy" or not would help to predict.
cor( DATA$boy,DATA$weight )
options(warn=0)
#################################################################
### Load infant weight data
###
load("./ClassMaterials/natalityNew.Rda")
###
### This creates a data frame called "d"
### This data set has 198377 observations
### 19 variables
#################################################################
### Cleaning up the Data
###
### We need to clean up a little
### lets look at a summary
summary(d)
### we see that:
### tri.none is all zeros so we will remove it.
### birmon is all 6 so we will remove it.
### Also, there are redundancies (and identifier number).
### We will remove the following
drops <- c("id","birmon","tri.none","novisit")
### This creates a dataframe (DATA) from d without the columns in drops
DATA <- d[,!(names(d) %in% drops)]
###
### Now we can see the new summary of the data
summary(DATA)
###
##################################################################
### Question 1 - Visualization
###
### We will consider visualizing:
### smoking patterns during pregnancy vs. education
###
### Education variables: DATA$ed.hs, DATA$ed.smcol, DATA$ed.col,
### Smoking Data: DATA$smoke DATA$cigsper
###
### Even simple things help us getting started to find what we want
### to do.
### First what is the proportion of mothers that smoke?
mean(DATA$smoke)
### Now lets see how this proportion breaks down by education.
### No High school
mean(DATA$smoke[DATA$ed.hs+DATA$ed.smcol+DATA$ed.col==0])
### With High school
mean(DATA$smoke[DATA$ed.hs>0])
### With Some College
mean(DATA$smoke[DATA$ed.smcol>0])
### With College
mean(DATA$smoke[DATA$ed.col>0])
### Clearly an interesting decreasing pattern.
### Arguably expected but the quantification of it is interesting
### Lets create a single variable that c
### Clearly an interesting decreasing pattern.
### Arguably expected but the quantification of it is interesting
### Lets create a single variable that codes all education levels
### so we can plot.
Education <- rep(0,length(DATA$smoke)) ### Code no HS as 0
Education[DATA$ed.hs>0]    <- 1        ### Code HS    as 1
Education[DATA$ed.smcol>0] <- 2        ### Code SMCOL as 2
Education[DATA$ed.col>0]   <- 3        ### Code COL   as 3
### Read as factors (and not numbers)
Education <- factor(Education, levels=c(0,1,2,3))
### Associate Labels with each level of the factors
levels(Education) <- c("No High School","High School","Some College","College")
### this call of "plot" displays the proportion of smokers broken down by education
par(mar=c(1.5,1.5,1.5,1.5))
par(mai=c(1.5,1.5,1.5,1.5))
plot(factor(DATA$smoke) ~ Education, col=c(8,2), ylab="Smoking")
###########################################
###########################################
### Question 3 - In your opinion, can any of the variables provided
### in Exhibit 1 help to predict birthweight?
### Provide (a simple) evidence that supports your view based on data.
###
### Yes! It sonds very likely that some of these variables we
### will have some explanatory value. Simple measure: correlation.
### It seems reasonable that knowing if it is a "Boy" or not would help to predict.
cor( DATA$boy,DATA$weight )
### Yes! Correlation 0.1. It definitely has some predictive power.
### Technically this already drives the point home.
###
### Lets compute the correlation matrix (just need to call cor with the dataframe)
CorMatrix <- cor(DATA)
### the command CorMatrix <- cor(DATA, DATA) would work in the same way
### First Column of that matrix corresponds to weight vs. all variables:
CorMatrix[,1]
### Although small, all variables seem to be correlated with weight.
###
###
summary( glm( DATA$weight ~ DATA$boy ) )
### the command CorMatrix <- cor(DATA, DATA) would work in the same way
### First Column of that matrix corresponds to weight vs. all variables:
CorMatrix[,1]
### Although small, all variables seem to be correlated with weight.
###
###
summary( glm( DATA$weight ~ DATA$boy ) )
corrplot(CorMatrix, method = "square")
corrplot(CorMatrix, method = "ellipse")
corrplot(CorMatrix, method = "circle")
corrplot(CorMatrix, method = "square")
##########################################################################
### Question 4 Run a linear regression
###
### The response variable Y will be "weight" (dependent, target)
### we will add all the other variables in the regression.
result <- glm(weight ~ ., data = DATA)
###
### Lets see the summary of the model with coef and p-values
summary(result)
### All p-values are very significative! Largest p-value is 3.84e-05.
### Both rules accept all coefficient
###
### For the conservative rule, significance is
### p-value <= 0.05 / # of tests
### then the following command counts hot many were not significant
sum( summary(result)$coef[,4] > 0.05/length(summary(result)$coef[,4]) )
1 - (model2$dev/modelq2$null)
model2 <- ctree(cost~., data = data)
model2 <- ctree(cost~., data = data[,-c(2,6)])
model2pred <- predict (model2, newdata = new.customers)
model2pred
1 - (model2$dev/modelq2$null)
model1 <- glm(cost ~., data = testCorr)
summary(model1)
model1 <- glm(cost ~., data = testCorr[,-2])
summary(model1)
corrplot(Corr, method = "square", col = rev(CorrplotColor), tl.col = "black")
summary(model1)
model1 <- glm(cost ~., data = testCorr[,-c(2,12)])
summary(model1)
model1 <- glm(cost ~., data = testCorr[,-c(1,2,12)])
summary(model1)
model1 <- glm(cost ~., data = testCorr[,-c(1,2,12,13)])
summary(model1)
1 - (model1$dev/model1$null)
model1 <- glm(cost ~.+(A+B+C+D+E+F+G)^2, data = testCorr[,-c(1,2,12,13)])
model1 <- glm(cost ~.+(A+B+C+D+E+F+G)^2, data = testCorr)
summary(model1)
1 - (model1$dev/model1$null)
model1 <- glm(cost ~.+(A+B+C+D+E+F+G)^2, data = testCorr)
summary(model1)
model1 <- glm(cost ~.+(A+B+C+D+E+F+G)^2, data = testCorr[,-c(1,2)])
summary(model1)
1 - (model1$dev/model1$null)
1 - (model1$dev/model1$null)
model1 <- glm(cost ~.+(A+B+C+D+E+F+G)^2, data = testCorr)
summary(model1)
1 - (model1$dev/model1$null)
summary(model1)
modelq2 <- glm(cost ~. , data = data)
summary(modelq2)
modelq2pred <- predict(modelq2, newdata = new.customers)
1 - (modelq2$dev/modelq2$null)
modelq2pred <- predict(modelq2, newdata = new.customers)
model1 <- glm(cost ~.+(A+B+C+D+E+F+G)^2, data = data)
summary(model1)
1 - (model1$dev/model1$null)
model1 <- glm(cost ~.+(A+B+C+D+E+F+G)^2, data = data)
summary(model1)
1 - (model1$dev/model1$null)
modelq2 <- glm(cost ~. , data = data)
summary(modelq2)
modelq2pred <- predict(modelq2, newdata = new.customers)
1 - (modelq2$dev/modelq2$null)
corrplot(Corr, method = "square", col = rev(CorrplotColor), tl.col = "black")
# predict
Prediction1 <- predict(model1, newdata = new.customers)
# Its kinda heard  here, but i think we have to usilize all the functions he set up for us. That will make the mining process easier.
# And also this should use the methods tought in class 4.... but i'm still not getting it.
Prediction1
Predictionq2
Predictionq2 <- predict(modelq2, newdata = new.customers)
# Its kinda heard  here, but i think we have to usilize all the functions he set up for us. That will make the mining process easier.
# And also this should use the methods tought in class 4.... but i'm still not getting it.
Prediction1
Predictionq2
# Its kinda heard  here, but i think we have to usilize all the functions he set up for us. That will make the mining process easier.
# And also this should use the methods tought in class 4.... but i'm still not getting it.
Prediction1
Predictionq2
model1<-tree(cost~., data = data)
model2 <- ctree(cost~., data = data[,-c(2,6)])
model2pred <- predict (model2, newdata = new.customers)
model3 <- rpart(cost~., data = data)
model3pred <- predict (model3, newdata = new.customers)
modelq2pred
model2pred
model3pred
model2 <- ctree(cost~.+(A+B+C+D+E+F+G)^2, data = data[,-c(2,6)])
model2pred <- predict (model2, newdata = new.customers)
model2pred
model2 <- ctree(cost~., data = data[,-c(2,6)])
model2 <- ctree(cost~., data = data[,-c(2,6)])
model2pred <- predict (model2, newdata = new.customers)
model2pred <- predict (model2, newdata = new.customers)
model2pred
model3 <- rpart(cost~., data = data)
model3pred <- predict (model3, newdata = new.customers)
model3pred
plot(model2)
model3pred
plot(model2)
plot(model3)
text(model3,label="yval")
text(model3, use.n = TRUE, all = TRUE, cex = 0.8)
data("mtcars")  # 使用示例数据
model3 <- rpart(mpg ~ hp + wt, data = mtcars)
plot(model3)
text(model3, use.n = TRUE, all = TRUE, cex = 0.8)
plot(model3,text(model3, use.n = TRUE, all = TRUE, cex = 0.8)
)
packageVersion("rpart")
install.packages("randomForest")
library(randomForest)
model4 <- randomForest(cost~., data=data, nodesize=5, ntree = 500, mtry = 4)
data_nona <- na.omit(data)
model4 <- randomForest(cost~., data=data_nona, nodesize=5, ntree = 500, mtry = 4)
model4pred <- predict(model4, newdata = new.customers)
model4pred
nrow(data)
nrow(data)*80%
nrow(data)*0.80
nrow(data)
nrow(data) * 0.8
data[1:nrow(data) * 0.8,]
data[1:as.integer(nrow(data) * 0.8),]
data-training_data
training_data <- data[1:as.integer(nrow(data) * 0.8),]
data-training_data
data[-training_data,]
data[-c(1:as.integer(nrow(data) * 0.8),]
data[-c(1:as.integer(nrow(data) * 0.8)),]
as.integer(nrow(data)*0.2
as.integer(nrow(data)*0.2
as.integer(nrow(data))*0.2
data[-c(1:as.integer(nrow(data) * 0.8)),]
as.integer(nrow(data))*0.2
training_data <- data[1:as.integer(nrow(data) * 0.8),]
testing_data <- data[-c(1:as.integer(nrow(data) * 0.8)),]
testing_model1 <- glm(cost ~.+(A+B+C+D+E+F+G)^2, data = training_data)
testing_model3 <- rpart(cost~., data = training_data)
testing_model4 <- randomForest(cost~., data=training_data, nodesize=5, ntree = 500, mtry = 4)
testing_model4 <- randomForest(cost~., data=na.omit(training_data), nodesize=5, ntree = 500, mtry = 4)
testPred1 <- predict(testing_model1, newdata = testing_data)
testPred3 <- predict(testing_model3, newdata = testing_data)
testPred4 <- predict(testing_model4, newdata = testing_data)
testPred1
testPred1
testPred3
testPred4
training_data <- data_nona[1:as.integer(nrow(data_nona) * 0.8),]
testing_data <- data_nona[-c(1:as.integer(nrow(data_nona) * 0.8)),]
testing_model1 <- glm(cost ~.+(A+B+C+D+E+F+G)^2, data = training_data)
testing_model3 <- rpart(cost~., data = training_data)
testing_model4 <- randomForest(cost~., data=training_data, nodesize=5, ntree = 500, mtry = 4)
testPred1 <- predict(testing_model1, newdata = testing_data)
testPred3 <- predict(testing_model3, newdata = testing_data)
testPred4 <- predict(testing_model4, newdata = testing_data)
testPred1
testPred4
testing_data %>% mutate(testPredict = testPred1)
Test1 <- testing_data %>% mutate(testPredict = testPred1)
Test3 <- testing_data %>% mutate(testPredict = testPred3)
Test4 <- testing_data %>% mutate(testPredict = testPred4)
deviance(Test1$cost,Test1$testPredict, family="gaussian")
R2(Test1$cost, Test1$testPredict, family = "gaussian")
R2(Test1$cost, Test1$testPredict, family = "gaussian")
R2(Test3$cost, Test3$testPredict, family = "gaussian")
R2(Test4$cost, Test4$testPredict, family = "gaussian")
R2(Test1$cost, Test1$testPredict, family = "gaussian")
R2(Test3$cost, Test3$testPredict, family = "gaussian")
R2(Test4$cost, Test4$testPredict, family = "gaussian")
model1pred
# predict
model1pred <- predict(model1, newdata = new.customers)
model1pred
model3pred
RegressionModel <- R2(Test1$cost, Test1$testPredict, family = "gaussian") # 0.4515599
CART <- R2(Test3$cost, Test3$testPredict, family = "gaussian") # 0.3683401
RandomForest <- R2(Test4$cost, Test4$testPredict, family = "gaussian") # 0.368745
OOS_Perform <- data.frame(Model = c("RegressionModel", "CART", "RandomForest"),
R2 = c(RegressionModel, CART, RandomForest))
OOS_Perform
ggplot(OOS_Perform)+
geom_bar(aes(y = R2))
ggplot(OOS_Perform)+
geom_col(aes(y = R2))
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2))
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2))+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), color = "lightblue2")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightblue2")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightblue3")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightskyblue3")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
xlim(0, 0.6)
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
ylim(0, 0.6)+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)", labels = R2)+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
geom_text(aes(x = Model, y = R2, labels = R2))+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
geom_text(aes(x = Model, y = R2, label = R2))+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
geom_text(aes(x = Model, y = R2, label = round(R2,2)))+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
geom_text(aes(x = Model, y = R2+4, label = round(R2,2)))+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
geom_text(aes(x = Model, y = R2+1, label = round(R2,2)))+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
geom_text(aes(x = Model, y = R2+0.1, label = round(R2,2)))+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
geom_text(aes(x = Model, y = R2+0.01, label = round(R2,2)))+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
geom_text(aes(x = Model, y = R2+0.05, label = round(R2,2)))+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
geom_text(aes(x = Model, y = R2+0.03, label = round(R2,2)))+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)")+
theme_bw()
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
geom_text(aes(x = Model, y = R2+0.03, label = round(R2,3)))+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)")+
theme_bw()
data
mean(data$cost)
mean(data_nona$cost)
mean(data$cost)
mean(data_nona$cost)
sd(data$cost)
sd(data_nona$cost)
sd(data$cost)
sd(data_nona$cost)
# Choosing model through OOS
training_data <- data_nona[1:as.integer(nrow(data_nona) * 0.8),]
testing_data <- data_nona[-c(1:as.integer(nrow(data_nona) * 0.8)),]
### train model
testing_model1 <- glm(cost ~.+(A+B+C+D+E+F+G)^2, data = training_data)
testing_model2 <- ctree(cost~., data = training_data[,-c(2,6)])
#DECISION TREE
library(tree)
library('sandwich')
library('party')
library('rpart')
library('rpart')
testing_model2 <- ctree(cost~., data = training_data[,-c(2,6)])
testing_model3 <- rpart(cost~., data = training_data)
testing_model4 <- randomForest(cost~., data=training_data, nodesize=5, ntree = 500, mtry = 4)
#RANDOM FOREST
library(randomForest)
testing_model4 <- randomForest(cost~., data=training_data, nodesize=5, ntree = 500, mtry = 4)
### Predict Using testing data as newdata
testPred1 <- predict(testing_model1, newdata = testing_data)
testPred2 <- predict(testing_model2, newdata = testing_data)
testPred3 <- predict(testing_model3, newdata = testing_data)
testPred4 <- predict(testing_model4, newdata = testing_data)
### Compare with actual data
Test1 <- testing_data %>% mutate(testPredict = testPred1)
##################
###import data ###
##################
library(readxl)
library(readr)
library(tidyverse)
library(ggplot2)
library(stringr)
source("DataAnalyticsFunctions.R")
source("DataAnalyticsFunctions.R")
### Compare with actual data
Test1 <- testing_data %>% mutate(testPredict = testPred1)
Test2 <- testing_data %>% mutate(testPredict = testPred2)
Test3 <- testing_data %>% mutate(testPredict = testPred3)
Test4 <- testing_data %>% mutate(testPredict = testPred4)
### R2 calculating and plotting
RegressionModel <- R2(Test1$cost, Test1$testPredict, family = "gaussian") # 0.4515599
CART_ctree <- R2(Test2$cost, Test2$testPredict, family = "gaussian") # 0.207664
CART_rpart <- R2(Test3$cost, Test3$testPredict, family = "gaussian") # 0.3683401
RandomForest <- R2(Test4$cost, Test4$testPredict, family = "gaussian") # 0.368745
OOS_Perform <- data.frame(Model = c("RegressionModel", "CART_ctree", "CART_rpart", "RandomForest"),
R2 = c(RegressionModel, CART_ctree, CART_rpart, RandomForest))
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
geom_text(aes(x = Model, y = R2+0.03, label = round(R2,3)))+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)")+
theme_bw()
###############################
# Expected Return
compare <- data_nona$cost-predict(model1, newdata = data_nona)
###############################
# Expected Return
compare <- data_nona$cost-predict(model1, newdata = data_nona)
compare <- data.frame(diff = compare)
compare$diff <- as.numeric(compare$diff)
compare <- compare %>% mutate(smaller = compare$diff > 0,) %>% cbind(data_nona)
sd <- sd(compare$diff)
n <- length(compare$diff)
se <- sd/sqrt(n)
ERplot <- data.frame(x = seq(-1,-300, by = -1))
ERplot <- ERplot %>% mutate(tstat = (x-mean(compare$diff))/sd,
P_Win = 1-pnorm(tstat, lower.tail = TRUE),
OurQuote1 = model1pred[1] + x,
OurQuote2 = model1pred[2] + x,
OurQuote3 = model1pred[3] + x)
ERplot <- ERplot %>% mutate(ExpectReturn = (OurQuote1+OurQuote2+OurQuote3)*P_Win)
ERplot %>% filter(ExpectReturn == max(ExpectReturn))
ggplot(data = ERplot)+
geom_line(aes(x = x, y = ExpectReturn))+
geom_text(aes(x = -66, y = 1600, label = "1639.678"))+
geom_text(aes(x = -66, y = 500, label = "x= -66"))+
ylim(500,2000)+
theme_bw()
ourQuote <- model1pred -66
ourQuote
model1pred
model2pred #ctree
model3pred #rpart
model4pred
ggplot(OOS_Perform)+
geom_col(aes(x = Model, y = R2), fill = "lightsteelblue3")+
geom_text(aes(x = Model, y = R2+0.03, label = round(R2,3)))+
ylim(0, 0.6)+
labs(title = "OOS Performance (R2)")+
theme_bw()
library(readxl)
library(readr)
library(readr)
library(tidyverse)
library(ggplot2)
library(stringr)
source("DataAnalyticsFunctions.R")
data <- read_csv("https://raw.githubusercontent.com/hereisjulia/DataScience_MQM/main/Case2/ALLSTATEcost.csv")
data <- read_csv("https://raw.githubusercontent.com/hereisjulia/DataScience_MQM/main/Case2/ALLSTATEcost.csv")
View(data)
data[, names(data)! %in% drop]
data[, !names(data) %in% drop]
data <- data[, !names(data) %in% drop]
ggplot(data = data)+
geom_point(aes(x = car_age, y = cost))
ggplot(data = data)+
geom_point(aes(x = car_age, y = cost, color = car_value))
ggplot(data = data)+
geom_jitter(aes(x = car_age, y = cost, color = car_value))
ggplot(data = data)+
geom_jitter(aes(x = car_value, y = cost, color = car_age))
