library(readr)
library(tidyverse)
library(ggplot2)
library(stringr)
data <- read_csv("https://raw.githubusercontent.com/hereisjulia/DataScience_MQM/main/TermProject/Loan_default.csv")
data_drop <- c("LoanID")
data_clean <- data[, !names(data) %in% data_drop]
data_clean$HasMortgage <- ifelse(data_clean$HasMortgage == "Yes", 1, 0)
data_clean$HasDependents <- ifelse(data_clean$HasDependents == "Yes", 1, 0)
data_clean$HasCoSigner <- ifelse(data_clean$HasCoSigner == "Yes", 1, 0)
education_mapping <- c("High School"= 1, "Bachelor's" = 2, "Master's" = 3, "PhD" = 4)
data_clean$Education <- as.integer(factor(data_clean$Education, levels = names(education_mapping)))
EmploymentType_mapping <- c("Unemployed" = 0, "Self-employed" = 1,"Part-time" = 2, "Full-time" = 3)
data_clean$EmploymentType <- as.integer(factor(data_clean$EmploymentType, levels = names(EmploymentType_mapping)))
MaritalStatus_Mapping <- c("Single" = 0, "Divorced" = 1, "Married" = 2)
data_clean$MaritalStatus <- as.integer(factor(data_clean$MaritalStatus, levels = names(MaritalStatus_Mapping)))
LoanPurpose_mapping <- c("Other" = 0, "Auto" = 1, "Education" = 2, "Home" = 3, "Business" = 4)
data_clean$LoanPurpose <- as.integer(factor(data_clean$LoanPurpose, levels = names(LoanPurpose_mapping)))
length(data_clean)
data_clean
Mx<- model.matrix(Default ~ ., data=data_clean)[,-1]
My<- data_clean$Default
num.features <- ncol(Mx)
num.n <- nrow(Mx)
num.default <- sum(My)
w <- (num.default/num.n)*(1-(num.default/num.n))
lambda.theory <- sqrt(w*log(num.features/0.05)/num.n)
lassoTheory <- glmnet(Mx,My, family="binomial",lambda = lambda.theory)
library(readr)
library(tidyverse)
library(ggplot2)
library(stringr)
library(RColorBrewer)
library(corrplot)
library(glmnet)
num.features <- ncol(Mx)
num.n <- nrow(Mx)
num.default <- sum(My)
w <- (num.default/num.n)*(1-(num.default/num.n))
lambda.theory <- sqrt(w*log(num.features/0.05)/num.n)
lassoTheory <- glmnet(Mx,My, family="binomial",lambda = lambda.theory)
summary(lassoTheory)
support(lassoTheory$beta)
if(x %in% rownames(installed.packages())==FALSE) {
if(x %in% rownames(available.packages())==FALSE) {
paste(x,"is not a valid package - please check again...")
} else {
install.packages(x)
}
} else {
paste(x,"package already installed...")
}
installpkg <- function(x){
if(x %in% rownames(installed.packages())==FALSE) {
if(x %in% rownames(available.packages())==FALSE) {
paste(x,"is not a valid package - please check again...")
} else {
install.packages(x)
}
} else {
paste(x,"package already installed...")
}
}
### FPR_TPR
FPR_TPR <- function(prediction, actual){
TP <- sum((prediction)*(actual))
FP <- sum((prediction)*(!actual))
FN <- sum((!prediction)*(actual))
TN <- sum((!prediction)*(!actual))
result <- data.frame( FPR = FP / (FP + TN), TPR = TP / (TP + FN), ACC = (TP+TN)/(TP+TN+FP+FN) )
return (result)
}
BinaryAccuracy <- function(prediction, actual){
TP <- sum((prediction)*(actual))
FP <- sum((prediction)*(!actual))
FN <- sum((!prediction)*(actual))
TN <- sum((!prediction)*(!actual))
result <-  (TP+TN)/(TP+TN+FP+FN)
return (result)
}
### Returns the indices for which |x[i]| > tr
support<- function(x, tr = 10e-6) {
m<- rep(0, length(x))
for (i in 1:length(x)) if( abs(x[i])> tr ) m[i]<- i
m <- m[m>0]
m
}
### Penalty choice for Quantile Regression
lambda.BC<- function(X, R = 1000, tau = 0.5, c = 1, alpha = .05){
n <- nrow(X)
sigs <- apply(X,2,norm2n)
U <- matrix(runif(n * R),n)
R <- (t(X) %*% (tau - (U < tau)))/(sigs*sqrt(tau*(1-tau)))
r <- apply(abs(R),2,max)
c * quantile(r, 1 - alpha) * sqrt(tau*(1-tau))*c(1,sigs)
}
### Used in Class 5, Class 9 and Online Test 5
## Selects the Number of Clusters via an Information Criteria
## get AIC (option "A") and BIC (option "B") for the output of kmeans
kIC <- function(fit, rule=c("A","B")){
df <- length(fit$centers) # K*dim
n <- sum(fit$size)
D <- fit$tot.withinss # deviance
rule=match.arg(rule)
if(rule=="A")
#return(D + 2*df*n/max(1,n-df-1))
return(D + 2*df)
else
return(D + log(n)*df)
}
num.features <- ncol(Mx)
num.n <- nrow(Mx)
num.default <- sum(My)
w <- (num.default/num.n)*(1-(num.default/num.n))
lambda.theory <- sqrt(w*log(num.features/0.05)/num.n)
lassoTheory <- glmnet(Mx,My, family="binomial",lambda = lambda.theory)
summary(lassoTheory)
support(lassoTheory$beta)
colnames(Mx)[support(lassoTheory$beta)]
### there are in total
length(support(lassoTheory$beta))
lassoCV <- cv.glmnet(Mx,My, family="binomial")
summary(lassoCV)
plot(lassoCV, main="Fitting Graph for CV Lasso \n \n # of non-zero coefficients  ", xlab = expression(paste("log(",lambda,")")))
optimal_lambda <- lassoCV$lambda.min
lasso_optLambda <- glmnet(Mx,My, family="binomial",lambda = optimal_lambda)
length(support(lasso_optLambda$beta))
select_variables <- names(data_clean)[support(lasso_optLambda$beta)]
selected_vars <- paste(select_variables, collapse = " + ")
formula <- as.formula(paste("Default ~", selected_vars))
